{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it5tLAhw44AC"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gR5Zuv82Yp1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3LJH7zCdr4s"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import random  \n",
        "import string\n",
        "from string import punctuation\n",
        "import bs4 as bs  \n",
        "import urllib.request  \n",
        "import re\n",
        "import collections\n",
        "import operator\n",
        "import multiprocessing\n",
        "import gensim.models.word2vec as w2v\n",
        "import os\n",
        "import warnings\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueoWyY1KCTNw"
      },
      "source": [
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o69FruoU1UAF"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bDy4w8R1S1O",
        "outputId": "833c0713-bbe7-4bf6-e909-b0399d178bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "docs = [\n",
        "  'I took the dog dog for a walk',\n",
        "  'I went for a walk with my dog last evening',\n",
        "  'I went to a movie last evening',\n",
        "]\n",
        "\n",
        "## Step 1: Determine the Vocabulary\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(docs)\n",
        "print(f'Vocabulary: {list(tokenizer.word_index.keys())}')\n",
        "\n",
        "## Step 2: Count\n",
        "vectors = tokenizer.texts_to_matrix(docs, mode='count')\n",
        "print(vectors) # First element is always 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: ['i', 'dog', 'a', 'for', 'walk', 'went', 'last', 'evening', 'took', 'the', 'with', 'my', 'to', 'movie']\n",
            "[[0. 1. 2. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPh0XC8UYZ4T"
      },
      "source": [
        "## Text processing and TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9HGs_Q824kc"
      },
      "source": [
        "# fetch the content from Internet by providing URL\n",
        "\n",
        "def get_article(url):\n",
        "  url_read = urllib.request.urlopen(url)\n",
        "  raw_html = url_read.read()\n",
        "\n",
        "  article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "  article_paragraphs = article_html.find_all('p')\n",
        "  article_text = ''\n",
        "\n",
        "  for para in article_paragraphs:  \n",
        "      article_text += para.text\n",
        "\n",
        "  article = nltk.sent_tokenize(article_text)\n",
        "\n",
        "  return article  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWHbzE908W3X"
      },
      "source": [
        "# process the text - lower case, remove spaces, punctuations and stopwords\n",
        " \n",
        "def get_TF(text):\n",
        "  punc = string.punctuation\n",
        "  words = nltk.word_tokenize(' '.join(text).lower())\n",
        "\n",
        "  words = [w for w in words if not w in punc]\n",
        "  words = [w for w in words if not w in stopwords.words(\"english\")]\n",
        "  words = [re.sub(r'\\s+',' ',w) for w in words]\n",
        "\n",
        "  cv = CountVectorizer()\n",
        "  vec = cv.fit_transform(words)\n",
        "  count_list = vec.toarray().sum(axis = 0)\n",
        "  word_list = cv.get_feature_names()\n",
        "  vocab = sorted(dict(zip(word_list, count_list)).items(),key=operator.itemgetter(1), reverse=True) \n",
        "\n",
        "  return vocab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2IuHZadBQJy"
      },
      "source": [
        "# Provide URL, fetch articles, get TF of the entire vocabulary used\n",
        "\n",
        "doc1 = get_article('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "doc2 = get_article('https://indianexpress.com/article/sports/football/fa-cup-final-2020-arsenal-vs-chelsea-live-score-updates-6534611/')\n",
        "vocab1 = get_TF(doc1)\n",
        "vocab2 = get_TF(doc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zJ2nRJ60RK",
        "outputId": "652f6a46-6fc1-4748-b8a0-92b5176fda6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "# See the article fetched - how many sentences and the article\n",
        "\n",
        "import textwrap\n",
        "print(len(doc1))\n",
        "slice = doc1[0:20]\n",
        "textwrap.wrap(''.join(slice), width = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing (NLP) is a subfield of linguistics, computer science, information',\n",
              " 'engineering, and artificial intelligence concerned with the interactions between computers and human',\n",
              " '(natural) languages, in particular how to program computers to process and analyze large amounts of',\n",
              " 'natural language data.Challenges in natural language processing frequently involve speech',\n",
              " 'recognition, natural language understanding, and natural-language generation.The history of natural',\n",
              " 'language processing (NLP) generally started in the 1950s, although work can be found from earlier',\n",
              " 'periods.In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\"',\n",
              " 'which proposed what is now called the Turing test as a criterion of intelligence[clarification',\n",
              " 'needed].The Georgetown experiment in 1954 involved fully automatic translation of more than sixty',\n",
              " 'Russian sentences into English.The authors claimed that within three or five years, machine',\n",
              " 'translation would be a solved problem.[2]  However, real progress was much slower, and after the',\n",
              " 'ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the',\n",
              " 'expectations, funding for machine translation was dramatically reduced.Little further research in',\n",
              " 'machine translation was conducted until the late 1980s when the first statistical machine',\n",
              " 'translation systems were developed.Some notably successful natural language processing systems',\n",
              " 'developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\"',\n",
              " 'with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by',\n",
              " 'Joseph Weizenbaum between 1964 and 1966.Using almost no information about human thought or emotion,',\n",
              " 'ELIZA sometimes provided a startlingly human-like interaction.When the \"patient\" exceeded the very',\n",
              " 'small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head',\n",
              " 'hurts\" with \"Why do you say your head hurts?\".During the 1970s, many programmers began to write',\n",
              " '\"conceptual ontologies\", which structured real-world information into computer-understandable',\n",
              " 'data.Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin',\n",
              " '(Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert',\n",
              " '1981).During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.Up to',\n",
              " 'the 1980s, most natural language processing systems were based on complex sets of hand-written',\n",
              " 'rules.Starting in the late 1980s, however, there was a revolution in natural language processing',\n",
              " 'with the introduction of machine learning algorithms for language processing.This was due to both',\n",
              " \"the steady increase in computational power (see Moore's law) and the gradual lessening of the\",\n",
              " 'dominance of Chomskyan theories of linguistics (e.g.transformational grammar), whose theoretical',\n",
              " 'underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning',\n",
              " 'approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as',\n",
              " 'decision trees, produced systems of hard if-then rules similar to existing hand-written',\n",
              " 'rules.However, part-of-speech tagging introduced the use of hidden Markov models to natural language',\n",
              " 'processing, and increasingly, research has focused on statistical models, which make soft,',\n",
              " 'probabilistic decisions based on attaching real-valued weights to the features making up the input',\n",
              " 'data.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DyA9jx9-2xF"
      },
      "source": [
        "# See words and frequency\n",
        "\n",
        "vocab2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcjUxFsUdQyt"
      },
      "source": [
        "## TF - IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrEBl8uKdTTZ"
      },
      "source": [
        "# Get TFIDF of two docs fetched previously\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([' '.join(doc1), ' '.join(doc2)])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm2bIiLKIxSf"
      },
      "source": [
        "# Get TFIDF score of each document based on the query\n",
        "\n",
        "def get_doc(query):\n",
        "  tfidf_doc1, tfidf_doc2 = 0., 0.\n",
        "  query = nltk.word_tokenize(query.lower())\n",
        "  columns = df.columns\n",
        "  #print(query, columns)\n",
        "  for x in query:\n",
        "    #print(x)\n",
        "    if x in columns:\n",
        "      #print(x)\n",
        "      tfidf_doc1 += np.array(df[x])[0]\n",
        "      #print(tfidf_doc1)\n",
        "      tfidf_doc2 += np.array(df[x])[1]\n",
        "\n",
        "  return tfidf_doc1, tfidf_doc2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSIMkm4IP7yZ"
      },
      "source": [
        "  # Enter the query here\n",
        "\n",
        "  query = 'i want to learn machine learning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aurRUtoTTBeF",
        "outputId": "71d74bc6-bede-41c0-bc28-e20f3c7d4941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "  # Show which document to fetch based on the query above\n",
        "\n",
        "  tfidf_doc1, tfidf_doc2 = get_doc(query)\n",
        "  docid = [1 if tfidf_doc1>= tfidf_doc2 else 2]\n",
        "  #print('  {:20s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "  print('Show document', docid,'\\n')\n",
        "  print('TF-IDF of doc1 = %0.4f'%(tfidf_doc1))\n",
        "  print('TF-IDF of doc2 = %0.4f'%(tfidf_doc2),'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Show document [1] \n",
            "\n",
            "TF-IDF of doc1 = 0.4532\n",
            "TF-IDF of doc2 = 0.1714 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq38ix5-aMw_",
        "outputId": "d2d88be4-9a6b-4f05-c6aa-29ef24248ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "  # See data in the dataframe\n",
        "\n",
        "  col = set(word_tokenize(query)).intersection(set(df.columns))\n",
        "  df[col]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>machine</th>\n",
              "      <th>learning</th>\n",
              "      <th>learn</th>\n",
              "      <th>to</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.123699</td>\n",
              "      <td>0.108237</td>\n",
              "      <td>0.023194</td>\n",
              "      <td>0.198029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.171440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    machine  learning     learn        to\n",
              "0  0.123699  0.108237  0.023194  0.198029\n",
              "1  0.000000  0.000000  0.000000  0.171440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVIgp2z8mv6D"
      },
      "source": [
        "## Word to Vector - word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiZI-JO3m2jx",
        "outputId": "90a539d1-e5c1-48bf-bec6-5e99ae4e7f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "per1 = np.array([90, 60, 75, 88, 98, 45]).reshape(1,-1)\n",
        "per2 = np.array([87, 65, 55, 90, 95, 35]).reshape(1,-1)\n",
        "per3 = np.array([40, 77, 95, 38, 48, 95]).reshape(1,-1)\n",
        "print(cosine_similarity(per1, per2),cosine_similarity(per1, per3), cosine_similarity(per2, per3)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.99343892]] [[0.84337893]] [[0.80204826]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I66vUhIr-ZNu"
      },
      "source": [
        "fileObject = open(\"/content/1-18 books combined.txt\", \"r\")\n",
        "mb = fileObject .read()\n",
        "mb_sent = sent_tokenize(mb)\n",
        "print(mb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VetmeEbAioW"
      },
      "source": [
        "print(mb_sent[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geNh7Dv3BSnm"
      },
      "source": [
        "def sent2word(sentences):\n",
        "  words = []\n",
        "\n",
        "  for raw_sentence in sentences:\n",
        "    if len(raw_sentence) > 0:\n",
        "      words.append(word_tokenize(raw_sentence))\n",
        "  return words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXW7hap_Bw-I"
      },
      "source": [
        "sentences = sent2word(mb_sent)\n",
        "sentences[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwk4a7YSCC8P"
      },
      "source": [
        "token_count = sum([len(sentence) for sentence in sentences])\n",
        "print(\"The book corpus contains {0:,} tokens.\".format(token_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15W0zvsxCUQX"
      },
      "source": [
        "##Build Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KPqS9zECY4X"
      },
      "source": [
        "num_features = 500\n",
        "min_word_count = 6\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "context_size = 15\n",
        "downsampling = 1e-3\n",
        "seed = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2Rvc_QiC1Uu"
      },
      "source": [
        "mb2vec = w2v.Word2Vec(\n",
        "    sg=1,\n",
        "    seed=seed,\n",
        "    workers=num_workers,\n",
        "    size=num_features,\n",
        "    min_count=min_word_count,\n",
        "    window=context_size,\n",
        "    sample=downsampling\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htha_nfSDKv0"
      },
      "source": [
        "mb2vec.build_vocab(sentences)\n",
        "print(\"Word2Vec vocabulary length:\", len(mb2vec.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBAb5TTMDt0k"
      },
      "source": [
        "mb2vec.train(sentences, total_examples=mb2vec.corpus_count, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiyB4GbHEW2l"
      },
      "source": [
        "mb2vec.most_similar('Draupadi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7W2YU5PEk25"
      },
      "source": [
        "def nearest_similarity_cosmul(start1, end1, start2):\n",
        "    '''Find the word that completes the relationship.'''\n",
        "    similarities = mb2vec.most_similar_cosmul(\n",
        "        positive=[start1, start2],\n",
        "        negative=[end1])\n",
        "    end2 = similarities[0][0]\n",
        "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
        "    return end2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtDyC1p5EnTT"
      },
      "source": [
        "nearest_similarity_cosmul(\"Dhritarastra\" ,\"Pandu\", \"Nakula\")  \n",
        "nearest_similarity_cosmul(\"Bhima\" ,\"Arjuna\", \"Ambika\") \n",
        "nearest_similarity_cosmul(\"Karna\" ,\"Duryodhana\", \"Kunti\")\n",
        "nearest_similarity_cosmul(\"Bhima\" ,\"Draupadi\", \"Ulupi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeQxLkaAFHkU"
      },
      "source": [
        "mb2vec.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r81VS8ZUFfjb"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNfe7wbMFkQX"
      },
      "source": [
        "if not os.path.exists(\"trained\"):\n",
        "  os.makedirs(\"trained\")\n",
        "mb2vec.save(os.path.join(\"trained\", \"mb2vec.w2v\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppRF0ncbF08i"
      },
      "source": [
        "mb2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"mb2vec.w2v\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnCtxnL4I99R"
      },
      "source": [
        "## GloVe - the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3-nb4AQXefu",
        "outputId": "bfc24c16-89f6-4c70-90dc-24792e12a167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!pip install glove_python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700224 sha256=3601c6c5f07ba5c5d9a0984c1aec7abe41ead4694a727dd8b333169a1b60010e\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG6gdJA9X2UF",
        "outputId": "366b8d65-acbe-4fbe-ae48-d7773bf594c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "corp_glove = ['Hello this is a tutorial on how to convert the word in an integer format', 'this is a beautiful day','Jack is going to office']\n",
        "corp_sent = sent2word(corp_glove)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3a79de244c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorp_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Hello this is a tutorial on how to convert the word in an integer format'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'this is a beautiful day'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Jack is going to office'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorp_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorp_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sent2word' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjpZzkc0cMZo"
      },
      "source": [
        "corp_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ-kTy0-YO5I"
      },
      "source": [
        "from glove import Corpus, Glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ZywscCZLKL"
      },
      "source": [
        "corpus = Corpus()\n",
        "corpus.fit(corp_sent, window=10)\n",
        "glove = Glove(no_components=5, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhKqWyqbc65E"
      },
      "source": [
        "print (glove.word_vectors[glove.dictionary['tutorial']])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VvKY0LvkG9S"
      },
      "source": [
        "embeddings_dict = {}\n",
        "with open('/content/glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "        embeddings_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf2tAX1nFDog"
      },
      "source": [
        "def get_embedding(word):\n",
        "  word = word.lower()\n",
        "  if word in embeddings_dict.keys():\n",
        "    return embeddings_dict[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_DOS94UIIFt"
      },
      "source": [
        "get_embedding('kailash')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1JkwLwmkvJ4"
      },
      "source": [
        "def find_closest_embeddings(embedding):\n",
        "  distance = [(spatial.distance.euclidean(embeddings_dict[word], embeddings_dict[embedding]) for word in embeddings_dict.keys())]\n",
        "  #return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RAN75GoLuSa"
      },
      "source": [
        "distance_list = []\n",
        "word_list = []\n",
        "for word in embeddings_dict.keys():\n",
        "  distance = (spatial.distance.euclidean(embeddings_dict[word], embeddings_dict['king']))\n",
        "  if distance < 4.0:\n",
        "    distance_list.append(distance)\n",
        "    word_list.append(word)\n",
        "\n",
        "print(word_list)\n",
        "print(distance_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCXdIg1YPFln",
        "outputId": "b46377b5-f0ef-4685-c57b-1cb2e1c0c741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-100\")  # download the model and return as object ready for use\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEMIegfLlVLd",
        "outputId": "d4e88b2d-382d-4817-bf09-320c32211bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "# Get embeddings\n",
        "model['samsung']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.20165 , -0.10063 ,  0.70741 ,  0.38328 ,  0.26653 , -0.73444 ,\n",
              "        0.18056 ,  0.10257 ,  0.36666 , -0.10203 ,  0.74039 ,  0.064539,\n",
              "        0.17015 , -0.3904  , -0.028034,  0.010385,  0.16209 ,  0.45656 ,\n",
              "        1.1704  , -0.081035,  0.9173  , -0.87593 ,  0.025113,  1.2481  ,\n",
              "       -0.11057 ,  0.14544 ,  0.013   ,  0.47959 , -0.37063 , -0.20684 ,\n",
              "        0.15821 ,  0.64398 , -0.53477 ,  0.050977,  0.42249 , -0.31015 ,\n",
              "        0.5536  ,  0.28029 , -0.14466 , -0.55467 ,  0.31696 ,  0.067902,\n",
              "       -0.48795 , -0.10996 , -0.86542 ,  1.0242  ,  0.67504 ,  0.77755 ,\n",
              "        0.30693 , -0.79164 , -0.34635 , -0.83695 , -0.36242 , -0.52085 ,\n",
              "        0.18611 , -0.23385 , -1.1157  , -0.25272 ,  0.70889 , -0.017324,\n",
              "       -0.040643, -0.37613 , -0.42664 ,  0.51648 , -0.33523 ,  0.092375,\n",
              "        0.53513 ,  1.2845  ,  0.18675 ,  1.005   ,  0.75454 ,  0.56702 ,\n",
              "       -0.45096 , -0.83243 , -0.92564 ,  0.83302 ,  0.53917 ,  0.095942,\n",
              "       -0.11653 ,  0.47449 ,  2.1476  , -0.33144 ,  0.33733 ,  0.58529 ,\n",
              "       -0.94161 , -0.30504 , -0.10221 ,  0.22397 ,  1.2913  ,  0.21552 ,\n",
              "        0.17347 ,  0.64245 ,  0.13847 ,  0.1652  ,  0.86132 , -0.10426 ,\n",
              "       -0.6703  , -0.55919 ,  1.3571  , -0.14713 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtjHf2GGXGa8"
      },
      "source": [
        "# Get similar words\n",
        "def similar_words(word):\n",
        "  word = [item.lower() for item in word ]\n",
        "  return (model.most_similar(positive = word, topn= 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRz7exz8Zf0Y",
        "outputId": "c694b700-21c7-455b-f5ee-19046dfa0fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "# Get similar words\n",
        "word = ['gachibowli']\n",
        "output = similar_words(word)\n",
        "\n",
        "for i, k in enumerate(output):\n",
        "  a, b = k\n",
        "  print ('{:4s} {:10s} : {:0.4f},'.format(repr(i),repr(a.title()), b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    'Fedexfield' : 0.7650,\n",
            "1    'Madhapur' : 0.7549,\n",
            "2    'Barabati' : 0.7416,\n",
            "3    'Higashi-Ku' : 0.7330,\n",
            "4    'Kingsmeadow' : 0.7311,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUdalkfjFNXO",
        "outputId": "dfd331c7-0f96-4dfa-b0ea-b2fc2a2e1e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Finding the odd one out\n",
        "model.doesnt_match(['mango','banana', 'apricot','peach','guava'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'banana'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baf9fbUIT-kj"
      },
      "source": [
        "## Wordnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbl4hULKVpc5"
      },
      "source": [
        "poses = {'n':'noun', 'v': 'verb', 's':'adj (s)','a':'adj', 'r':'adv'}\n",
        "for synset in wn.synsets('good'):\n",
        "  print('{}:{}'.format(poses[synset.pos()],'.'.join([l.name() for l in synset.lemmas()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT0wQiCWKF5"
      },
      "source": [
        "panda = (wn.synset('panda.n.01'))\n",
        "hyper = lambda s: s.hypernyms()\n",
        "list1 = panda.closure(hyper)\n",
        "print(list1.synset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXPD5QI3XdMA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpbGn79E48Bo"
      },
      "source": [
        "## Understanding RNN Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWVp9wVxyLFR"
      },
      "source": [
        "class myRNNCell(tf.keras.layers.Layer):\n",
        "  def __init__(self, rnn_units. input_dim, output_dim):\n",
        "    super(myRNNCell, self).__init__()\n",
        "\n",
        "    # initialise weight metrices\n",
        "    self.W_xh = self.add_weight([rnn_units, input_dim])\n",
        "    self.W_hh = self.add_weight([rnn_units, rnn_units])\n",
        "    self.W_hy = self.add_weight([output_dim, rnn_units])\n",
        "\n",
        "    # initialise hidden states to zeros\n",
        "    self.h = np.zeros([rnn_units, 1])\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    #update hidden state\n",
        "    self.h = tf.math.tanh(self.W_hh * self.h + self.W_xh * x)\n",
        "\n",
        "    #compute output\n",
        "    output = self.W_hy * self.h\n",
        "\n",
        "    #return the current output and hidden state\n",
        "    return output, self.h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPW3NDmNqb07"
      },
      "source": [
        "# above code is same as\n",
        "myRNN = tf.keras.layers.SimpleRNN(rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}